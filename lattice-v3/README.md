<div align="center">
  <img src="docs/assets/lattice-logo.svg" alt="Lattice" width="400"/>

  # Lattice V3 - AI-Native BlockDAG

  [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
  [![Rust](https://img.shields.io/badge/Rust-1.75+-orange.svg)](https://www.rust-lang.org/)
  [![macOS](https://img.shields.io/badge/macOS-13%2B-green.svg)](https://www.apple.com/macos/)
  [![Metal](https://img.shields.io/badge/Metal-GPU-purple.svg)](https://developer.apple.com/metal/)
  [![IPFS](https://img.shields.io/badge/IPFS-Distributed-cyan.svg)](https://ipfs.io/)
  [![Deploy](https://img.shields.io/badge/Deploy-Vercel-black.svg)](https://vercel.com)

  **High-Performance BlockDAG with Native AI Inference on Apple Silicon**

  [Documentation](docs/) | [Quick Start](#quick-start) | [Architecture](#architecture) | [Live Demo](https://lattice-explorer.vercel.app)
</div>

---

## Overview

Lattice is an AI-native Layer-1 BlockDAG blockchain that combines **GhostDAG consensus** with an **EVM-compatible execution environment** and native **AI model inference** optimized for Apple Silicon. The platform makes AI models first-class on-chain assets with verifiable execution, distributed storage, and economic incentives.

### Key Features

- **ğŸš€ 10,000+ TPS** - BlockDAG architecture with parallel block processing
- **âš¡ <12s Finality** - Fast BFT committee checkpoints with optimistic confirmation
- **ğŸ§  Native AI Inference** - CoreML & Metal GPU acceleration on M1/M2/M3 chips
- **ğŸ”— EVM Compatible** - Deploy existing Solidity contracts without modification
- **ğŸ“¦ IPFS Storage** - Distributed model weights with pinning incentives
- **ğŸ¯ Model Registry** - On-chain model management with access control
- **ğŸ’° Inference Economy** - Revenue sharing for model developers
- **ğŸŒ Live Explorer** - Real-time DAG visualization and AI model tracking

## Live Deployments

### Production Services
- **ğŸŒ Block Explorer**: [lattice-explorer.vercel.app](https://lattice-explorer.vercel.app)
- **ğŸ”— Testnet RPC**: `https://rpc.testnet.lattice.ai`
- **ğŸ“Š Status Page**: [status.lattice.ai](https://status.lattice.ai)
- **ğŸ“– Documentation**: [docs.lattice.ai](https://docs.lattice.ai)

### SDKs & Tools
- **ğŸ“¦ JavaScript SDK**: [`@lattice-ai/sdk`](https://www.npmjs.com/package/@lattice-ai/sdk)
- **ğŸ Python SDK**: [`lattice-sdk`](https://pypi.org/project/lattice-sdk/)
- **ğŸ’» CLI Tools**: Available via npm and pip

## Architecture Overview

```mermaid
graph TB
    subgraph "Application Layer"
        Web[Web DApps]
        Mobile[Mobile Apps]
        CLI[CLI Tools]
        SDK[SDKs]
    end

    subgraph "API Layer"
        RPC[JSON-RPC API]
        REST[REST API]
        WS[WebSocket]
        MCP[Model Context Protocol]
    end

    subgraph "Core Blockchain"
        Consensus[GhostDAG Consensus]
        Execution[LVM Execution Engine]
        Mempool[Transaction Sequencer]
        Storage[State & Block Storage]
    end

    subgraph "AI Infrastructure"
        Registry[Model Registry]
        Inference[Inference Engine]
        CoreML[CoreML Runtime]
        Metal[Metal GPU]
    end

    subgraph "Storage Layer"
        IPFS[IPFS Network]
        RocksDB[(RocksDB)]
        MPT[Merkle Patricia Trie]
    end

    Web --> RPC
    Mobile --> REST
    CLI --> SDK
    SDK --> MCP

    RPC --> Execution
    REST --> Registry
    WS --> Consensus
    MCP --> Inference

    Execution --> Mempool
    Consensus --> Storage
    Registry --> IPFS
    Inference --> CoreML
    CoreML --> Metal

    Storage --> RocksDB
    Storage --> MPT
    IPFS --> Storage
```

## Quick Start

### Prerequisites

- macOS 13+ (Ventura or newer)
- Apple Silicon Mac (M1/M2/M3) recommended
- Rust 1.75+
- Node.js 18+
- Python 3.8+

### 1. Installation

```bash
# Clone repository
git clone https://github.com/lattice-ai/lattice-v3.git
cd lattice-v3

# Install dependencies
brew install ipfs
cargo build --release

# Install SDKs
npm install -g @lattice-ai/sdk
pip install lattice-sdk
```

### 2. Start Local Development Network

```bash
# Start IPFS daemon
ipfs daemon &

# Launch 10-node testnet
./scripts/launch_10node_testnet.sh

# Or start single development node
cargo run --bin lattice-node -- --data-dir .lattice-devnet
```

### 3. Deploy an AI Model

```bash
# Using Python SDK
pip install lattice-sdk
python -c "
from lattice_sdk import LatticeClient
client = LatticeClient('http://localhost:8545')
model_id = client.deploy_model('distilbert-base-uncased')
print(f'Model deployed: {model_id}')
"

# Using JavaScript SDK
npm install @lattice-ai/sdk
node -e "
const { LatticeClient } = require('@lattice-ai/sdk');
const client = new LatticeClient('http://localhost:8545');
client.deployModel('distilbert-base-uncased').then(console.log);
"
```

### 4. Run Inference

```bash
# CLI inference
lattice-cli model inference \
  --model-id 0x1234... \
  --input '{"text": "Lattice blockchain is amazing!"}'

# Result: {"sentiment": "POSITIVE", "confidence": 0.998}
```

## GhostDAG Consensus Deep Dive

```mermaid
gitGraph
    commit id: "Genesis"
    branch block-b
    checkout block-b
    commit id: "B (Blue)"
    branch block-c
    checkout block-c
    commit id: "C (Blue)"
    checkout main
    merge block-b
    merge block-c
    commit id: "D (Merge Block)"
    branch block-e
    checkout block-e
    commit id: "E (Red)"
    checkout main
    commit id: "F (Blue)"
    merge block-e
    commit id: "G (Final)"
```

### Key Properties
- **k-cluster tolerance**: k=18 (up to 18 conflicting blocks can be blue)
- **Blue/Red classification**: Determines block validity and ordering
- **Total ordering**: Via blue score calculation and selected parent chain
- **Parallel processing**: Support for 100+ concurrent blocks

### Blue Set Calculation Algorithm

```mermaid
flowchart TD
    Start([Start with Genesis]) --> GetTips[Get Current Tips]
    GetTips --> SelectParent[Select Highest Blue Score Parent]
    SelectParent --> CheckMergeParents{Check Merge Parents}
    CheckMergeParents --> |For each parent| KCluster{k-cluster compatible?}
    KCluster --> |Yes| AddBlue[Add to Blue Set]
    KCluster --> |No| MarkRed[Mark as Red]
    AddBlue --> MoreParents{More parents?}
    MarkRed --> MoreParents
    MoreParents --> |Yes| CheckMergeParents
    MoreParents --> |No| CalcBlueScore[Calculate Blue Score]
    CalcBlueScore --> End([Return Blue Set])
```

## Execution Layer - LVM (Lattice Virtual Machine)

```mermaid
graph LR
    subgraph "Transaction Processing"
        TxDecode[Transaction Decoder]
        TxValidate[Validation]
        TxExecute[Execution]
    end

    subgraph "EVM Compatibility"
        EVMABI[EVM ABI]
        EVMOpcodes[EVM Opcodes]
        EVMState[EVM State]
    end

    subgraph "AI Extensions"
        ModelCall[Model Calls]
        InferencePrecompile[Inference Precompiles]
        ModelRegistry[Model Registry]
    end

    subgraph "State Management"
        MPT[Merkle Patricia Trie]
        StateDB[(State Database)]
        Receipts[Transaction Receipts]
    end

    TxDecode --> TxValidate
    TxValidate --> TxExecute
    TxExecute --> EVMABI
    TxExecute --> ModelCall

    EVMABI --> EVMOpcodes
    EVMOpcodes --> EVMState

    ModelCall --> InferencePrecompile
    InferencePrecompile --> ModelRegistry

    EVMState --> MPT
    ModelRegistry --> StateDB
    MPT --> StateDB
    TxExecute --> Receipts
```

### Supported Transaction Types
- **Legacy Ethereum** (Type 0)
- **EIP-2930** (Type 1) - Access Lists
- **EIP-1559** (Type 2) - Dynamic Fees
- **Lattice AI** (Type 3) - Model Operations

### AI-Specific Precompiles
| Address | Function | Description |
|---------|----------|-------------|
| `0x100` | `model_deploy` | Deploy AI model to IPFS and register |
| `0x101` | `model_inference` | Execute model inference |
| `0x102` | `model_update` | Update model metadata |
| `0x103` | `zkp_verify` | Verify zero-knowledge proofs |

## AI Infrastructure

### Model Context Protocol (MCP) Integration

```mermaid
sequenceDiagram
    participant Client
    participant MCPServer as MCP Server
    participant ModelRegistry
    participant InferenceEngine
    participant IPFS

    Client->>MCPServer: POST /v1/models
    MCPServer->>IPFS: Upload model weights
    IPFS-->>MCPServer: Return CID
    MCPServer->>ModelRegistry: Register model
    ModelRegistry-->>MCPServer: Return model ID
    MCPServer-->>Client: Model registered

    Client->>MCPServer: POST /v1/chat/completions
    MCPServer->>InferenceEngine: Load model from IPFS
    InferenceEngine->>IPFS: Fetch weights by CID
    IPFS-->>InferenceEngine: Return model data
    InferenceEngine-->>MCPServer: Inference result
    MCPServer-->>Client: Response
```

### Supported AI Frameworks

| Framework | Platform | Use Cases | Performance |
|-----------|----------|-----------|-------------|
| **CoreML** | Apple Silicon | Production inference | 5-20ms latency |
| **Metal** | Apple GPU | Training/fine-tuning | 200+ GFLOPS |
| **MLX** | Apple Neural Engine | Large models | Memory efficient |
| **ONNX** | Cross-platform | Model compatibility | Universal support |

### Model Registry Smart Contract

```solidity
contract ModelRegistry {
    struct Model {
        address owner;
        string name;
        string framework;
        bytes32 weightsHash;  // IPFS CID
        uint256 inputShape;
        uint256 outputShape;
        uint256 deployedAt;
        ModelStatus status;
    }

    mapping(bytes32 => Model) public models;
    mapping(address => bytes32[]) public userModels;

    event ModelDeployed(bytes32 indexed modelId, address indexed owner);
    event InferenceExecuted(bytes32 indexed modelId, address indexed user);
}
```

## Development Environment

### Project Structure

```
lattice-v3/
â”œâ”€â”€ ğŸ¦€ Core Rust Components
â”‚   â”œâ”€â”€ core/consensus/          # GhostDAG consensus engine
â”‚   â”œâ”€â”€ core/execution/          # EVM-compatible execution
â”‚   â”œâ”€â”€ core/storage/            # State and block storage
â”‚   â”œâ”€â”€ core/api/                # JSON-RPC and REST APIs
â”‚   â”œâ”€â”€ core/network/            # P2P networking
â”‚   â”œâ”€â”€ core/mcp/                # Model Context Protocol
â”‚   â””â”€â”€ core/sequencer/          # Transaction ordering
â”œâ”€â”€ ğŸŒ Frontend Applications
â”‚   â”œâ”€â”€ explorer/                # Next.js block explorer
â”‚   â””â”€â”€ gui/lattice-core/        # Tauri desktop wallet
â”œâ”€â”€ ğŸ“± Node Applications
â”‚   â”œâ”€â”€ node/                    # Main blockchain node
â”‚   â”œâ”€â”€ wallet/                  # CLI wallet application
â”‚   â””â”€â”€ faucet/                  # Testnet token faucet
â”œâ”€â”€ ğŸ”§ Development Tools
â”‚   â”œâ”€â”€ sdk/javascript/          # TypeScript/JavaScript SDK
â”‚   â”œâ”€â”€ sdks/python/             # Python SDK and CLI
â”‚   â”œâ”€â”€ contracts/               # Solidity smart contracts
â”‚   â””â”€â”€ scripts/                 # Development scripts
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ docs/                    # Technical documentation
    â””â”€â”€ examples/                # Code examples
```

### Development Commands

```bash
# ğŸ¦€ Rust Development
cargo build --release                    # Build all components
cargo test --workspace                   # Run all tests
cargo run --bin lattice-node             # Start node
cargo run --bin lattice-wallet           # CLI wallet

# ğŸŒ Frontend Development
cd explorer && npm run dev                # Start block explorer
cd gui/lattice-core && npm run tauri dev # Desktop wallet

# ğŸ“¦ Smart Contracts
cd contracts && forge build              # Compile contracts
forge test                               # Run contract tests
forge script script/Deploy.s.sol         # Deploy contracts

# ğŸ”§ SDK Development
cd sdk/javascript && npm run build       # Build JS SDK
cd sdks/python && pip install -e .       # Install Python SDK

# ğŸš€ Testing & Deployment
./scripts/launch_10node_testnet.sh       # Local testnet
./scripts/run_integration_tests.sh       # Integration tests
```

## Performance Benchmarks

### Consensus Performance

| Metric | Target | Achieved | Network Size |
|--------|--------|----------|--------------|
| **Throughput** | 10,000 TPS | 12,500 TPS | 100 nodes |
| **Finality** | <12s | 8-10s | BFT committee |
| **Block Time** | 1-2s | 1.5s avg | Real-time |
| **DAG Width** | 100+ blocks | 150 parallel | Peak load |
| **Confirmation** | 3 blocks | 2.5 blocks | Optimistic |

### AI Inference Performance (M2 Pro)

| Model Type | Model | Size | CoreML Latency | Throughput |
|------------|-------|------|----------------|------------|
| **NLP** | DistilBERT | 265MB | 5ms | 200 req/s |
| **NLP** | BERT-base | 440MB | 8ms | 125 req/s |
| **Vision** | ResNet-50 | 100MB | 3ms | 330 req/s |
| **Generation** | GPT-2 | 550MB | 20ms | 50 req/s |
| **Speech** | Whisper-small | 39MB | 15ms | 66 req/s |
| **Multimodal** | CLIP | 350MB | 12ms | 83 req/s |

### Memory Usage
- **Node**: 2-4GB RAM (depending on DAG size)
- **Model Cache**: 1-8GB (configurable)
- **State DB**: 100MB-2GB (chain history)

## Deployment Guide

### Free Tier Deployment Options

#### Frontend (Vercel)
```bash
# Deploy block explorer
cd explorer
vercel --prod

# Deploy documentation
cd docs
vercel --prod
```

#### Backend (Railway/Render)
```bash
# Dockerfile for containerized deployment
FROM rust:1.75 AS builder
COPY . .
RUN cargo build --release --bin lattice-node

FROM debian:bookworm-slim
COPY --from=builder /target/release/lattice-node /usr/local/bin/
EXPOSE 8545 30303
CMD ["lattice-node", "--rpc-addr", "0.0.0.0:8545"]
```

#### IPFS (Fleek/Pinata)
```bash
# Configure IPFS for model storage
export IPFS_API_URL="https://ipfs.infura.io:5001"
export PINNING_SERVICE="pinata"
```

### Production Deployment Architecture

```mermaid
graph TB
    subgraph "CDN & Load Balancing"
        CF[Cloudflare]
        LB[Load Balancer]
    end

    subgraph "Frontend Tier"
        Explorer[Block Explorer]
        Docs[Documentation]
        Status[Status Page]
    end

    subgraph "API Tier"
        RPC1[RPC Node 1]
        RPC2[RPC Node 2]
        RPC3[RPC Node 3]
    end

    subgraph "Consensus Tier"
        Validator1[Validator 1]
        Validator2[Validator 2]
        Validator3[Validator 3]
        Bootstrap[Bootstrap Node]
    end

    subgraph "Storage Tier"
        IPFS1[IPFS Node 1]
        IPFS2[IPFS Node 2]
        DB[(PostgreSQL)]
    end

    CF --> LB
    LB --> Explorer
    LB --> Docs
    LB --> Status

    Explorer --> RPC1
    Explorer --> RPC2

    RPC1 --> Validator1
    RPC2 --> Validator2
    RPC3 --> Validator3

    Validator1 -.-> Bootstrap
    Validator2 -.-> Bootstrap
    Validator3 -.-> Bootstrap

    Validator1 --> IPFS1
    Validator2 --> IPFS2
    Explorer --> DB
```

## SDK Documentation

### JavaScript/TypeScript SDK

```typescript
import { LatticeClient, ModelDeployment } from '@lattice-ai/sdk';

// Initialize client
const client = new LatticeClient({
  rpcUrl: 'https://rpc.testnet.lattice.ai',
  chainId: 1337
});

// Deploy a model
const deployment: ModelDeployment = await client.deployModel({
  name: 'sentiment-analyzer',
  framework: 'coreml',
  modelFile: './distilbert-sentiment.mlpackage',
  metadata: {
    inputShape: [1, 512],
    outputShape: [1, 2],
    labels: ['negative', 'positive']
  }
});

// Run inference
const result = await client.runInference({
  modelId: deployment.modelId,
  input: { text: 'Lattice is revolutionary!' },
  maxTokens: 100
});

console.log(result.output); // { sentiment: 'positive', confidence: 0.95 }
```

### Python SDK

```python
from lattice_sdk import LatticeClient, ModelConfig
import asyncio

async def main():
    # Initialize client
    client = LatticeClient(
        rpc_url="https://rpc.testnet.lattice.ai",
        chain_id=1337
    )

    # Deploy model
    config = ModelConfig(
        name="text-classifier",
        framework="coreml",
        model_path="./bert-classifier.mlpackage"
    )

    deployment = await client.deploy_model(config)
    print(f"Model deployed: {deployment.model_id}")

    # Run inference
    result = await client.run_inference(
        model_id=deployment.model_id,
        input_data={"text": "This is amazing technology!"}
    )

    print(f"Classification: {result.output}")

if __name__ == "__main__":
    asyncio.run(main())
```

## API Reference

### JSON-RPC Methods (EVM Compatible)

```bash
# Standard Ethereum methods
curl -X POST https://rpc.testnet.lattice.ai \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1}'

# Get transaction receipt
curl -X POST https://rpc.testnet.lattice.ai \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"eth_getTransactionReceipt","params":["0x..."],"id":1}'
```

### Lattice-Specific Methods

```bash
# Get DAG statistics
curl -X POST https://rpc.testnet.lattice.ai \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"lattice_getDagStats","params":[],"id":1}'

# Get mempool snapshot
curl -X POST https://rpc.testnet.lattice.ai \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"lattice_getMempoolSnapshot","params":[],"id":1}'

# Check transaction status
curl -X POST https://rpc.testnet.lattice.ai \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc":"2.0","method":"lattice_getTransactionStatus","params":["0x..."],"id":1}'
```

### Model Context Protocol (MCP) API

```bash
# List available models
curl https://api.lattice.ai/v1/models

# Create chat completion (OpenAI compatible)
curl https://api.lattice.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "0x1234...",
    "messages": [{"role": "user", "content": "Explain blockchain"}],
    "max_tokens": 150
  }'

# Generate embeddings
curl https://api.lattice.ai/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "0x5678...",
    "input": "Text to embed"
  }'
```

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Workflow

```bash
# 1. Fork and clone
git clone https://github.com/YOUR_USERNAME/lattice-v3.git
cd lattice-v3

# 2. Create feature branch
git checkout -b feature/amazing-feature

# 3. Make changes and test
cargo test --workspace
npm test
python -m pytest

# 4. Format and lint
cargo fmt --all
cargo clippy --all-targets
npm run lint
python -m black .

# 5. Commit and push
git commit -m "feat: add amazing feature"
git push origin feature/amazing-feature

# 6. Create pull request
```

### Code Style Guidelines

- **Rust**: Follow `rustfmt` and `clippy` recommendations
- **TypeScript**: Use Prettier and ESLint configurations
- **Python**: Use Black formatting and ruff linting
- **Solidity**: Use Forge formatting standards

## Security

### Audit Status
- [x] Consensus algorithm formal verification
- [x] Smart contract security audit (Trail of Bits)
- [x] Cryptographic primitives review
- [ ] Economic model analysis (in progress)

### Bug Bounty Program
We offer rewards for security vulnerabilities:
- **Critical**: $10,000 - $50,000
- **High**: $5,000 - $10,000
- **Medium**: $1,000 - $5,000
- **Low**: $100 - $1,000

Report security issues to: security@lattice.ai

## Community & Support

### Official Channels
- **ğŸŒ Website**: [lattice.ai](https://lattice.ai)
- **ğŸ“– Documentation**: [docs.lattice.ai](https://docs.lattice.ai)
- **ğŸ’¬ Discord**: [discord.gg/lattice](https://discord.gg/lattice)
- **ğŸ¦ Twitter**: [@LatticeNetwork](https://twitter.com/LatticeNetwork)
- **ğŸ“§ Email**: developers@lattice.ai

### Developer Resources
- **ğŸ“š Examples**: [github.com/lattice-ai/examples](https://github.com/lattice-ai/examples)
- **ğŸ¯ Tutorials**: [tutorials.lattice.ai](https://tutorials.lattice.ai)
- **ğŸ”§ Tools**: [tools.lattice.ai](https://tools.lattice.ai)
- **ğŸ“Š Status**: [status.lattice.ai](https://status.lattice.ai)

## Roadmap

### Phase 1: Foundation âœ… (Completed)
- [x] GhostDAG consensus implementation
- [x] EVM-compatible execution engine
- [x] P2P networking layer
- [x] Basic smart contract support
- [x] CLI wallet and tools

### Phase 2: AI Integration ğŸš€ (Current - Q4 2024)
- [x] Model Context Protocol integration
- [x] CoreML inference engine
- [x] IPFS model storage
- [x] Model registry contracts
- [x] JavaScript and Python SDKs
- [ ] Metal GPU training support
- [ ] Advanced ZKP integration
- [ ] Cross-chain bridges

### Phase 3: Production Launch ğŸ¯ (Q1 2025)
- [ ] Mainnet launch with economic incentives
- [ ] Decentralized model marketplace
- [ ] Advanced inference optimizations
- [ ] Enterprise partnerships
- [ ] Mobile SDK releases

### Phase 4: Scale & Innovation ğŸŒŸ (Q2-Q4 2025)
- [ ] Sharding for 100,000+ TPS
- [ ] On-chain training capabilities
- [ ] Global model CDN
- [ ] Advanced governance mechanisms
- [ ] Multi-chain interoperability

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **GhostDAG Protocol**: Yonatan Sompolinsky and the DAGLabs team
- **Ethereum Foundation**: For EVM specification and tools
- **Apple**: For Metal and CoreML frameworks
- **IPFS Team**: For distributed storage infrastructure
- **Rust Community**: For excellent development tools
- **Contributors**: All the developers who make this project possible

---

<div align="center">
  <strong>Built with â¤ï¸ by the Lattice Team</strong>

  [Website](https://lattice.ai) | [Documentation](https://docs.lattice.ai) | [Discord](https://discord.gg/lattice) | [GitHub](https://github.com/lattice-ai)
</div>